---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, welcome to my page! I am a Research Scientist at Meta. Prior to this, I was a postdoctoral associate at the [Department of Electrical and Computer Engineering](https://cse.umn.edu/ece), [University of Minnesota](https://twin-cities.umn.edu/), mentored by [Prof. Mingyi Hong](https://people.ece.umn.edu/~mhong/mingyi.html) and [Prof. Shuzhong Zhang](https://sites.google.com/umn.edu/shuzhong-zhang). I obtained my Ph.D. degree in Applied Mathematics at the [Department of Mathematics](https://math.ucdavis.edu/), [UC Davis](https://www.ucdavis.edu/), advised by [Prof. Krishna Balasubramanian](https://sites.google.com/view/kriznakumar/) and [Prof. Shiqian Ma](https://sqma.rice.edu/). Prior to this, I received my B.S. degree in Mathematics from [Zhejiang University](http://www.zju.edu.cn/english/). My CV is [here](https://JasonJiaxiangLi.github.io/files/CV_Jiaxiang_Li.pdf).

My research lies at the intersection of applied mathematics, optimization theory, and artificial intelligence, with a focus on developing principled algorithms that advance both theoretical foundations and practical applications in modern machine learning. Specifically, I work on:
- Algorithm design for gradient-based, gradient-free (zeroth-order), and primal–dual methods for large-scale nonconvex optimization, including problems on Riemannian manifolds.
- Convergence theory for deterministic and stochastic minimax and bilevel optimization, with applications to machine learning and operations research.
- Distributed, decentralized and federated optimization algorithms for training large-scale AI systems.
- Theoretical foundations of reinforcement learning, especially policy-based methods, and their role in aligning large language models (LLMs).
- Efficient pre-training and fine-tuning methods for LLMs, bridging optimization principles with practical deployment.

## News
------

<div style="height: 400px; overflow-y: auto; border: 1px solid #e1e5e9; padding: 20px; border-radius: 8px; background-color: #f8f9fa; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
  <ul style="margin: 0; padding-left: 0; list-style: none;">
    
- August 2025: Starting my new job as Research Scientist at Meta!

- February 2025: paper "*Problem-Parameter-Free Decentralized Nonconvex Stochastic Optimization*" is accepted by **Pacific Journal of Optimization**.

- January 2025: One paper is accepted by **ICLR 2025** (**Spotlight**). Congratulations to all my collaborators!
  - *Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback*

- January 2025: Paper "*Riemannian Bilevel Optimization*" is accepted by **Journal of Machine Learning Research**!

- November 2024: Paper "*A Riemannian ADMM*" is accepted by **Mathematics of Operations Research**!

- September 2024: Two papers are accepted by **NeurIPS 2024**. Congratulations to all my collaborators!
  - *Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment*
  - *SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining*

- August 2024: I'm very happy to receive the **INFORMS Computing Society Prize**!

- August 2024: Paper "*Zeroth-order Riemannian Averaging Stochastic Approximation Algorithms*" is accepted by **SIAM Journal on Optimization**!

- July 2024: A new grant “Bi-Level Optimization for Hierarchical Machine Learning Problems: Models, Algorithms and Applications” is awarded from **NSF**. I'm excited to be the co-PI of this project with Prof Hong!

- May 2024: Paper "*Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark*" is accepted by **ICML 2024**. Congratulations to all my collaborators!

</ul>
</div>

<div class="page__footer-copyright">&copy; {{ site.time | date: '%Y' }} {{ site.name | default: site.title }}. {{ site.data.ui-text[site.locale].powered_by | default: "Powered by" }} <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
