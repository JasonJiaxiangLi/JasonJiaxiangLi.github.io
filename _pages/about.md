---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, welcome to my page! I am a Research Scientist at Meta. Prior to this, I was a postdoctoral associate at the [Department of Electrical and Computer Engineering](https://cse.umn.edu/ece), [University of Minnesota](https://twin-cities.umn.edu/), mentored by [Prof. Mingyi Hong](https://people.ece.umn.edu/~mhong/mingyi.html) and [Prof. Shuzhong Zhang](https://sites.google.com/umn.edu/shuzhong-zhang). I obtained my Ph.D. degree in Applied Mathematics at the [Department of Mathematics](https://math.ucdavis.edu/), [UC Davis](https://www.ucdavis.edu/), advised by [Prof. Krishna Balasubramanian](https://sites.google.com/view/kriznakumar/) and [Prof. Shiqian Ma](https://sqma.rice.edu/). Prior to this, I received my B.S. degree in Mathematics from [Zhejiang University](http://www.zju.edu.cn/english/). My CV is [here](https://JasonJiaxiangLi.github.io/files/CV_Jiaxiang_Li.pdf).

My research lies at the intersection of applied mathematics, optimization theory, and artificial intelligence, with a focus on developing principled algorithms that advance both theoretical foundations and practical applications in modern machine learning. Specifically, I work on:
- Algorithm design for gradient-based, gradient-free (zeroth-order), and primalâ€“dual methods for large-scale nonconvex optimization, including problems on Riemannian manifolds.
- Convergence theory for deterministic and stochastic minimax and bilevel optimization, with applications to machine learning and operations research.
- Distributed, decentralized and federated optimization algorithms for training large-scale AI systems.
- Theoretical foundations of reinforcement learning, especially policy-based methods, and their role in aligning large language models (LLMs).
- Efficient pre-training and fine-tuning methods for LLMs, bridging optimization principles with practical deployment.

<hr style="border: none; border-top: 1px solid #ddd; margin: 2em 0;">

## News
<div style="height: 800px; overflow-y: auto; border: 1px solid #e1e5e9; padding: 20px; border-radius: 8px; background-color: #f8f9fa; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
  <ul style="margin: 0; padding-left: 0; list-style: none;">

<li style="margin-bottom: 12px; padding: 8px 0; border-bottom: 1px solid #e9ecef;">January 2026: Paper <em>Muon Outperforms Adam in Tail-End Associative Memory Learning</em> is accepted by <strong>ICLR 2026</strong>. Congratulations to all my collaborators!</li>
  
<li style="margin-bottom: 12px; padding: 8px 0; border-bottom: 1px solid #e9ecef;">August 2025: Starting my new job as Research Scientist at <strong>Meta</strong>!</li>

<li style="margin-bottom: 12px; padding: 8px 0; border-bottom: 1px solid #e9ecef;">February 2025: Paper <em>Problem-Parameter-Free Decentralized Nonconvex Stochastic Optimization</em> is accepted by <strong>Pacific Journal of Optimization</strong>.</li>

<li style="margin-bottom: 12px; padding: 8px 0; border-bottom: 1px solid #e9ecef;">January 2025: Paper <em>Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback</em> is accepted by <strong>ICLR 2025</strong> (<strong>Spotlight</strong>). Congratulations to all my collaborators!</li>

<li style="margin-bottom: 12px; padding: 8px 0; border-bottom: 1px solid #e9ecef;">January 2025: Paper <em>Riemannian Bilevel Optimization</em> is accepted by <strong>Journal of Machine Learning Research</strong>!</li>

<li style="margin-bottom: 12px; padding: 8px 0; border-bottom: 1px solid #e9ecef;">November 2024: Paper <em>A Riemannian ADMM</em> is accepted by <strong>Mathematics of Operations Research</strong>!</li>

<li style="margin-bottom:12px; padding:8px 0; border-bottom:1px solid #e9ecef;">
    September 2024: Two papers are accepted by <strong>NeurIPS 2024</strong>. Congratulations to all my collaborators!
    <ul style="margin:8px 0 0 20px; padding-left:0;">
      <li><em>Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment</em></li>
      <li><em>SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining</em></li>
    </ul>
  </li>

<li style="margin-bottom: 12px; padding: 8px 0; border-bottom: 1px solid #e9ecef;">August 2024: I'm very happy to receive the <strong>INFORMS Computing Society Prize</strong>!</li>

<li style="margin-bottom: 12px; padding: 8px 0; border-bottom: 1px solid #e9ecef;">August 2024: Paper <em>Zeroth-order Riemannian Averaging Stochastic Approximation Algorithms</em> is accepted by <strong>SIAM Journal on Optimization</strong>!</li>

<li style="margin-bottom: 12px; padding: 8px 0; border-bottom: 1px solid #e9ecef;">July 2024: A new grant <em>Bi-Level Optimization for Hierarchical Machine Learning Problems: Models, Algorithms and Applications</em> is awarded from <strong>NSF</strong>. I'm excited to be the co-PI of this project with Prof Hong!</li>

<li style="margin-bottom: 12px; padding: 8px 0; border-bottom: 1px solid #e9ecef;">May 2024: Paper <em>Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark</em> is accepted by <strong>ICML 2024</strong>. Congratulations to all my collaborators!</li>

</ul>
</div>

<div style="margin-top:30px; text-align:left; overflow:hidden;">
  <script type="text/javascript" id="clustrmaps" 
  src="//clustrmaps.com/map_v2.js?d=d2-9GfXbNZ_CT16pVbPIUgTFUp-1XfNBsc7byNJ2bas&cl=ffffff&w=200&h=120">
</script>
</div>
